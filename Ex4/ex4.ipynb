{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7057fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow import keras\n",
    "\n",
    "import konlpy\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "print(konlpy.__version__)\n",
    "print(gensim.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_test.txt')\n",
    "\n",
    "train_data.head()\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    #중복제거\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    #결측치제거\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data = test_data.dropna(how = 'any')\n",
    "\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "\n",
    "    #토큰화&불용어제거\n",
    "    for sentence in train_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence)\n",
    "        temp_X = [word for word in temp_X if not word in stopwords]\n",
    "        X_train.append(temp_X)\n",
    "\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence)\n",
    "        temp_X = [word for word in temp_X if not word in stopwords]\n",
    "        X_test.append(temp_X)\n",
    "    \n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4) #상위 9996\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter] #인덱스정의\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "    \n",
    "    #벡터화\n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "\n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "    \n",
    "    \n",
    "x_train, y_train, x_test, y_test, word_to_index = load_data(train_data, test_data)\n",
    "\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]\n",
    "\n",
    "#데이터 문장 리스트 생성\n",
    "total_data_text = list(x_train) + list(x_test)\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "#최대 길이\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print(f'tolerance: {np.sum(num_tokens < max_tokens) / len(num_tokens)}%')\n",
    "\n",
    "# Padding seq 생성\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre', \n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre', \n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print('pre:' ,x_train.shape)\n",
    "\n",
    "vocab_size = 10000  #어휘 사전의 크기(10000단어)\n",
    "word_vector_dim = 16  #워드 벡터의 차원 수\n",
    "\n",
    "#RNN\n",
    "rnn_model = tf.keras.Sequential()\n",
    "rnn_model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "rnn_model.add(tf.keras.layers.LSTM(8))\n",
    "rnn_model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "rnn_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))#최종 출력은 긍정/부정을 나타내는 1dim\n",
    "\n",
    "rnn_model.summary()\n",
    "\n",
    "#1-D CNN\n",
    "cnn_model = tf.keras.Sequential()\n",
    "cnn_model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "cnn_model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "cnn_model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "cnn_model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "cnn_model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "cnn_model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "cnn_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "cnn_model.summary()\n",
    "\n",
    "#GlobalMaxPooling1D\n",
    "gloMP_model = tf.keras.Sequential()\n",
    "gloMP_model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "gloMP_model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "gloMP_model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "gloMP_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "gloMP_model.summary()\n",
    "\n",
    "#validation set 10000건 분리\n",
    "x_val = x_train[:10000]   \n",
    "y_val = y_train[:10000]\n",
    "\n",
    "#나머지\n",
    "partial_x_train = x_train[10000:]  \n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)\n",
    "\n",
    "epochs=10 \n",
    "\n",
    "#RNN\n",
    "rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "rnn_history = rnn_model.fit(partial_x_train, partial_y_train, epochs=epochs,\n",
    "                    batch_size=512, validation_data=(x_val, y_val), verbose=1)\n",
    "#CNN\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "cnn_history = cnn_model.fit(partial_x_train, partial_y_train, epochs=epochs,\n",
    "                    batch_size=512, validation_data=(x_val, y_val), verbose=1)\n",
    "#GlobalMaxPooling1D\n",
    "gloMP_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "gloMP_history = gloMP_model.fit(partial_x_train, partial_y_train, epochs=epochs,\n",
    "                    batch_size=512, validation_data=(x_val, y_val), verbose=1)\n",
    "\n",
    "# 테스트셋으로 평가\n",
    "rnn_results = rnn_model.evaluate(x_test,  y_test, verbose=2)\n",
    "cnn_results = cnn_model.evaluate(x_test,  y_test, verbose=2)\n",
    "gloMP_results = gloMP_model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print('='*70)\n",
    "print('평가 결과')\n",
    "print('RNN : ', rnn_results)\n",
    "print('CNN : ', cnn_results)\n",
    "print('GlobalMaxPooling1D : ', gloMP_results)\n",
    "print('='*70)\n",
    "\n",
    "#그래프\n",
    "rnn_history_dict = rnn_history.history\n",
    "cnn_history_dict = cnn_history.history\n",
    "gloMP_history_dict = gloMP_history.history\n",
    "\n",
    "# RNN\n",
    "rnn_acc = rnn_history_dict['accuracy']\n",
    "rnn_val_acc = rnn_history_dict['val_accuracy']\n",
    "rnn_loss = rnn_history_dict['loss']\n",
    "rnn_val_loss = rnn_history_dict['val_loss']\n",
    "\n",
    "# 1-D CNN\n",
    "cnn_acc = cnn_history_dict['accuracy']\n",
    "cnn_val_acc = cnn_history_dict['val_accuracy']\n",
    "cnn_loss = cnn_history_dict['loss']\n",
    "cnn_val_loss = cnn_history_dict['val_loss']\n",
    "\n",
    "# GlobalMaxPooling1D\n",
    "gloMP_acc = gloMP_history_dict['accuracy']\n",
    "gloMP_val_acc = gloMP_history_dict['val_accuracy']\n",
    "gloMP_loss = gloMP_history_dict['loss']\n",
    "gloMP_val_loss = gloMP_history_dict['val_loss']\n",
    "\n",
    "# Training and validation loss\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 10))\n",
    "\n",
    "# RNN\n",
    "rnn_epochs = range(1, len(rnn_acc) + 1)\n",
    "ax[0].plot(rnn_epochs, rnn_loss, 'bo', label='RNN Training loss')\n",
    "ax[0].plot(rnn_epochs, rnn_val_loss, 'b', label='RNN Validation loss')\n",
    "# 1-D CNN\n",
    "cnn_epochs = range(1, len(rnn_acc) + 1)\n",
    "ax[1].plot(cnn_epochs, cnn_loss, 'bo', label='CNN Training loss')\n",
    "ax[1].plot(cnn_epochs, cnn_val_loss, 'b', label='CNN Validation loss')\n",
    "# GlobalMaxPooling1D\n",
    "gloMP_epochs = range(1, len(rnn_acc) + 1)\n",
    "ax[2].plot(gloMP_epochs, gloMP_loss, 'bo', label='GloMP Training loss')\n",
    "ax[2].plot(gloMP_epochs, gloMP_val_loss, 'b', label='GloMP Validation loss')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Training and validation accuracy\n",
    "# 그림 초기화\n",
    "plt.clf()   \n",
    "# Training and validation loss\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 10))\n",
    "\n",
    "# RNN\n",
    "ax[0].plot(rnn_epochs, rnn_acc, 'bo', label='RNN Training acc')\n",
    "ax[0].plot(rnn_epochs, rnn_val_acc, 'b', label='RNN Validation acc')\n",
    "# 1-D CNN\n",
    "ax[1].plot(cnn_epochs, cnn_acc, 'bo', label='CNN Training acc')\n",
    "ax[1].plot(cnn_epochs, cnn_val_acc, 'b', label='CNN Validation acc')\n",
    "# GlobalMaxPooling1D\n",
    "ax[2].plot(gloMP_epochs, gloMP_acc, 'bo', label='GloMP Training acc')\n",
    "ax[2].plot(gloMP_epochs, gloMP_val_acc, 'b', label='GloMP Validation acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#Embedding 레이어 분석\n",
    "model = rnn_model\n",
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)\n",
    "\n",
    "word2vec_file_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/ex04.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))\n",
    "\n",
    "# 단어 개수만큼 워드 벡터를 파일에 기록\n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()\n",
    "\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word_vectors['끝']\n",
    "vector\n",
    "\n",
    "\n",
    "word_vectors.similar_by_word(\"끝\")\n",
    "\n",
    "#한국어 Word2Vec 임베딩\n",
    "word2vec_file_path = os.getenv('HOME')+'/data/word2vec_ko.model'\n",
    "word2vec = gensim.models.Word2Vec.load(word2vec_file_path)\n",
    "word2vec.wv.similar_by_word(\"끝\")\n",
    "\n",
    "vocab_size = 10000    \n",
    "word_vector_dim = 100  \n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec.wv:\n",
    "        embedding_matrix[i] = word2vec.wv[index_to_word[i]]\n",
    "vocab_size = 10000    \n",
    "word_vector_dim = 100  \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  #카피한 임베딩을 여기서 활용\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   #trainable을 True로 주면 Fine-tuning\n",
    "model.add(keras.layers.LSTM(128))\n",
    "rnn_model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)\n",
    "\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bd1e63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
